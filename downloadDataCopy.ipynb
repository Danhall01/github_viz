{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "59efc37f-82a4-4238-81c1-f7185c692ad8",
    "_uuid": "6f418e0dcf4295996e1d0bc048356d7233c0da70",
    "execution": {
     "iopub.execute_input": "2024-04-28T14:42:41.411578Z",
     "iopub.status.busy": "2024-04-28T14:42:41.411156Z",
     "iopub.status.idle": "2024-04-28T14:42:42.175285Z",
     "shell.execute_reply": "2024-04-28T14:42:42.174237Z",
     "shell.execute_reply.started": "2024-04-28T14:42:41.411500Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# https://github.com/SohierDane/BigQuery_Helper\n",
    "\"\"\"\n",
    "Helper class to simplify common read-only BigQuery tasks.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from google.cloud import bigquery\n",
    "\n",
    "\n",
    "class BigQueryHelper(object):\n",
    "    \"\"\"\n",
    "    Helper class to simplify common BigQuery tasks like executing queries,\n",
    "    showing table schemas, etc without worrying about table or dataset pointers.\n",
    "\n",
    "    See the BigQuery docs for details of the steps this class lets you skip:\n",
    "    https://googlecloudplatform.github.io/google-cloud-python/latest/bigquery/reference.html\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, active_project, dataset_name, max_wait_seconds=180):\n",
    "        self.project_name = active_project\n",
    "        self.dataset_name = dataset_name\n",
    "        self.max_wait_seconds = max_wait_seconds\n",
    "        self.client = bigquery.Client()\n",
    "        self.__dataset_ref = self.client.dataset(self.dataset_name, project=self.project_name)\n",
    "        self.dataset = None\n",
    "        self.tables = dict()  # {table name (str): table object}\n",
    "        self.__table_refs = dict()  # {table name (str): table reference}\n",
    "        self.total_gb_used_net_cache = 0\n",
    "        self.BYTES_PER_GB = 2**30\n",
    "\n",
    "    def __fetch_dataset(self):\n",
    "        \"\"\"\n",
    "        Lazy loading of dataset. For example,\n",
    "        if the user only calls `self.query_to_pandas` then the\n",
    "        dataset never has to be fetched.\n",
    "        \"\"\"\n",
    "        if self.dataset is None:\n",
    "            self.dataset = self.client.get_dataset(self.__dataset_ref)\n",
    "\n",
    "    def __fetch_table(self, table_name):\n",
    "        \"\"\"\n",
    "        Lazy loading of table\n",
    "        \"\"\"\n",
    "        self.__fetch_dataset()\n",
    "        if table_name not in self.__table_refs:\n",
    "            self.__table_refs[table_name] = self.dataset.table(table_name)\n",
    "        if table_name not in self.tables:\n",
    "            self.tables[table_name] = self.client.get_table(self.__table_refs[table_name])\n",
    "\n",
    "    def __handle_record_field(self, row, schema_details, top_level_name=''):\n",
    "        \"\"\"\n",
    "        Unpack a single row, including any nested fields.\n",
    "        \"\"\"\n",
    "        name = row['name']\n",
    "        if top_level_name != '':\n",
    "            name = top_level_name + '.' + name\n",
    "        schema_details.append([{\n",
    "            'name': name,\n",
    "            'type': row['type'],\n",
    "            'mode': row['mode'],\n",
    "            'fields': pd.np.nan,\n",
    "            'description': row['description']\n",
    "                               }])\n",
    "        # float check is to dodge row['fields'] == np.nan\n",
    "        if type(row.get('fields', 0.0)) == float:\n",
    "            return None\n",
    "        for entry in row['fields']:\n",
    "            self.__handle_record_field(entry, schema_details, name)\n",
    "\n",
    "    def __unpack_all_schema_fields(self, schema):\n",
    "        \"\"\"\n",
    "        Unrolls nested schemas. Returns dataframe with one row per field,\n",
    "        and the field names in the format accepted by the API.\n",
    "        Results will look similar to the website schema, such as:\n",
    "            https://bigquery.cloud.google.com/table/bigquery-public-data:github_repos.commits?pli=1\n",
    "\n",
    "        Args:\n",
    "            schema: DataFrame derived from api repr of raw table.schema\n",
    "        Returns:\n",
    "            Dataframe of the unrolled schema.\n",
    "        \"\"\"\n",
    "        schema_details = []\n",
    "        schema.apply(lambda row:\n",
    "            self.__handle_record_field(row, schema_details), axis=1)\n",
    "        result = pd.concat([pd.DataFrame.from_dict(x) for x in schema_details])\n",
    "        result.reset_index(drop=True, inplace=True)\n",
    "        del result['fields']\n",
    "        return result\n",
    "\n",
    "    def table_schema(self, table_name):\n",
    "        \"\"\"\n",
    "        Get the schema for a specific table from a dataset.\n",
    "        Unrolls nested field names into the format that can be copied\n",
    "        directly into queries. For example, for the `github.commits` table,\n",
    "        the this will return `committer.name`.\n",
    "\n",
    "        This is a very different return signature than BigQuery's table.schema.\n",
    "        \"\"\"\n",
    "        self.__fetch_table(table_name)\n",
    "        raw_schema = self.tables[table_name].schema\n",
    "        schema = pd.DataFrame.from_dict([x.to_api_repr() for x in raw_schema])\n",
    "        # the api_repr only has the fields column for tables with nested data\n",
    "        if 'fields' in schema.columns:\n",
    "            schema = self.__unpack_all_schema_fields(schema)\n",
    "        # Set the column order\n",
    "        schema = schema[['name', 'type', 'mode', 'description']]\n",
    "        return schema\n",
    "\n",
    "    def list_tables(self):\n",
    "        \"\"\"\n",
    "        List the names of the tables in a dataset\n",
    "        \"\"\"\n",
    "        self.__fetch_dataset()\n",
    "        return([x.table_id for x in self.client.list_tables(self.dataset)])\n",
    "\n",
    "    def estimate_query_size(self, query):\n",
    "        \"\"\"\n",
    "        Estimate gigabytes scanned by query.\n",
    "        Does not consider if there is a cached query table.\n",
    "        See https://cloud.google.com/bigquery/docs/reference/rest/v2/jobs#configuration.dryRun\n",
    "        \"\"\"\n",
    "        my_job_config = bigquery.job.QueryJobConfig()\n",
    "        my_job_config.dry_run = True\n",
    "        my_job = self.client.query(query, job_config=my_job_config)\n",
    "        return my_job.total_bytes_processed / self.BYTES_PER_GB\n",
    "\n",
    "    def query_to_pandas(self, query):\n",
    "        \"\"\"\n",
    "        Execute a SQL query & return a pandas dataframe\n",
    "        \"\"\"\n",
    "        my_job = self.client.query(query)\n",
    "        start_time = time.time()\n",
    "        while not my_job.done():\n",
    "            if (time.time() - start_time) > self.max_wait_seconds:\n",
    "                print(\"Max wait time elapsed, query cancelled.\")\n",
    "                self.client.cancel_job(my_job.job_id)\n",
    "                return None\n",
    "            time.sleep(0.1)\n",
    "        # Queries that hit errors will return an exception type.\n",
    "        # Those exceptions don't get raised until we call my_job.to_dataframe()\n",
    "        # In that case, my_job.total_bytes_billed can be called but is None\n",
    "        if my_job.total_bytes_billed:\n",
    "            self.total_gb_used_net_cache += my_job.total_bytes_billed / self.BYTES_PER_GB\n",
    "        return my_job.to_dataframe()\n",
    "\n",
    "    def query_to_pandas_safe(self, query, max_gb_scanned=1):\n",
    "        \"\"\"\n",
    "        Execute a query, but only if the query would scan less than `max_gb_scanned` of data.\n",
    "        \"\"\"\n",
    "        query_size = self.estimate_query_size(query)\n",
    "        if query_size <= max_gb_scanned:\n",
    "            return self.query_to_pandas(query)\n",
    "        msg = \"Query cancelled; estimated size of {0} exceeds limit of {1} GB\"\n",
    "        print(msg.format(query_size, max_gb_scanned))\n",
    "\n",
    "    def head(self, table_name, num_rows=5, start_index=None, selected_columns=None):\n",
    "        \"\"\"\n",
    "        Get the first n rows of a table as a DataFrame.\n",
    "        Does not perform a full table scan; should use a trivial amount of data as long as n is small.\n",
    "        \"\"\"\n",
    "        self.__fetch_table(table_name)\n",
    "        active_table = self.tables[table_name]\n",
    "        schema_subset = None\n",
    "        if selected_columns:\n",
    "            schema_subset = [col for col in active_table.schema if col.name in selected_columns]\n",
    "        results = self.client.list_rows(active_table, selected_fields=schema_subset,\n",
    "            max_results=num_rows, start_index=start_index)\n",
    "        results = [x for x in results]\n",
    "        return pd.DataFrame(\n",
    "            data=[list(x.values()) for x in results], columns=list(results[0].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "4f2fee89-eb01-4ff5-962a-9aff7ab39169",
    "_uuid": "5cfce1b71299d690d85eed8b6fddd5cecbc18b42",
    "execution": {
     "iopub.execute_input": "2024-04-28T14:42:45.984769Z",
     "iopub.status.busy": "2024-04-28T14:42:45.984408Z",
     "iopub.status.idle": "2024-04-28T14:42:45.989438Z",
     "shell.execute_reply": "2024-04-28T14:42:45.988684Z",
     "shell.execute_reply.started": "2024-04-28T14:42:45.984716Z"
    }
   },
   "outputs": [],
   "source": [
    "bq_assistant = BigQueryHelper(\"bigquery-public-data\", \"github_repos\", 99999999999999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "c4fdf061-7c34-466e-a1f2-9a0455416f96",
    "_uuid": "c0e006696147d608460e8805e07b56b401762a21",
    "execution": {
     "iopub.execute_input": "2024-04-28T14:42:49.849733Z",
     "iopub.status.busy": "2024-04-28T14:42:49.849116Z",
     "iopub.status.idle": "2024-04-28T14:42:50.376804Z",
     "shell.execute_reply": "2024-04-28T14:42:50.375570Z",
     "shell.execute_reply.started": "2024-04-28T14:42:49.849628Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138.45546568464488\n"
     ]
    }
   ],
   "source": [
    "QUERY = \"\"\"\n",
    "        SELECT commits.message, commits.author, commits.repo_name[0] as repo, languages.language\n",
    "        FROM `bigquery-public-data.github_repos.commits` AS commits\n",
    "        LEFT JOIN `bigquery-public-data.github_repos.languages` AS languages\n",
    "        ON commits.repo_name[0] = languages.repo_name\n",
    "        WHERE commits.repo_name[0] IN \n",
    "            (SELECT DISTINCT repo_name[0] \n",
    "            FROM `bigquery-public-data.github_repos.commits`\n",
    "            WHERE ARRAY_LENGTH(repo_name)=1 and REGEXP_CONTAINS(message, '[a-zA-Z0-9]+([(].*[)]){0,1}!{0,1}:[ ]*.*$')\n",
    "            LIMIT 10000)\n",
    "        \"\"\"\n",
    "print(bq_assistant.estimate_query_size(QUERY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "e408843c-bdbb-437b-bea8-647d588ca8a9",
    "_uuid": "1e2bda49e999fef20715e4aee79d494d62a7c3fc",
    "execution": {
     "iopub.execute_input": "2024-04-27T12:35:15.930012Z",
     "iopub.status.busy": "2024-04-27T12:35:15.929634Z",
     "iopub.status.idle": "2024-04-27T13:16:33.980849Z",
     "shell.execute_reply": "2024-04-27T13:16:33.979908Z",
     "shell.execute_reply.started": "2024-04-27T12:35:15.929965Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15min 53s, sys: 10.4 s, total: 16min 4s\n",
      "Wall time: 38min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = bq_assistant.query_to_pandas_safe(QUERY, 140)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T13:17:27.715329Z",
     "iopub.status.busy": "2024-04-27T13:17:27.714965Z",
     "iopub.status.idle": "2024-04-27T13:17:33.181686Z",
     "shell.execute_reply": "2024-04-27T13:17:33.180679Z",
     "shell.execute_reply.started": "2024-04-27T13:17:27.715277Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dataframe: 2464727094 Bytes\n"
     ]
    }
   ],
   "source": [
    "print('Size of dataframe: {} Bytes'.format(int(df.memory_usage(index=True, deep=True).sum())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "31b51db3-4b28-42f2-8075-23f1b2137af9",
    "_uuid": "a3bbdbda054f32754b265bf1ab59638d1345f4dd",
    "execution": {
     "iopub.execute_input": "2024-04-27T13:19:04.904155Z",
     "iopub.status.busy": "2024-04-27T13:19:04.903781Z",
     "iopub.status.idle": "2024-04-27T13:19:04.926048Z",
     "shell.execute_reply": "2024-04-27T13:19:04.925125Z",
     "shell.execute_reply.started": "2024-04-27T13:19:04.904105Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>author</th>\n",
       "      <th>repo</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Small fixes to SignalR service extension metho...</td>\n",
       "      <td>{'name': 'James Newton-King', 'email': '474ba6...</td>\n",
       "      <td>aspnet/AspNetCore</td>\n",
       "      <td>[{'name': 'ASP', 'bytes': 109}, {'name': 'Batc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Added logging of .NET client HTTP requests (#1...</td>\n",
       "      <td>{'name': 'James Newton-King', 'email': '474ba6...</td>\n",
       "      <td>aspnet/AspNetCore</td>\n",
       "      <td>[{'name': 'ASP', 'bytes': 109}, {'name': 'Batc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Update gRPC package version to 2.23.2\\n</td>\n",
       "      <td>{'name': 'James Newton-King', 'email': '474ba6...</td>\n",
       "      <td>aspnet/AspNetCore</td>\n",
       "      <td>[{'name': 'ASP', 'bytes': 109}, {'name': 'Batc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Merge pull request #13 from ESSS/dev-instructi...</td>\n",
       "      <td>{'name': 'Tadeu Manoel', 'email': '4c402af6b06...</td>\n",
       "      <td>ESSS/esss_fix_format</td>\n",
       "      <td>[{'name': 'Batchfile', 'bytes': 34}, {'name': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Make trailers safer in TestHost (#32633)\\n\\n</td>\n",
       "      <td>{'name': 'James Newton-King', 'email': '474ba6...</td>\n",
       "      <td>aspnet/AspNetCore</td>\n",
       "      <td>[{'name': 'ASP', 'bytes': 109}, {'name': 'Batc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             message  \\\n",
       "0  Small fixes to SignalR service extension metho...   \n",
       "1  Added logging of .NET client HTTP requests (#1...   \n",
       "2            Update gRPC package version to 2.23.2\\n   \n",
       "3  Merge pull request #13 from ESSS/dev-instructi...   \n",
       "4       Make trailers safer in TestHost (#32633)\\n\\n   \n",
       "\n",
       "                                              author                  repo  \\\n",
       "0  {'name': 'James Newton-King', 'email': '474ba6...     aspnet/AspNetCore   \n",
       "1  {'name': 'James Newton-King', 'email': '474ba6...     aspnet/AspNetCore   \n",
       "2  {'name': 'James Newton-King', 'email': '474ba6...     aspnet/AspNetCore   \n",
       "3  {'name': 'Tadeu Manoel', 'email': '4c402af6b06...  ESSS/esss_fix_format   \n",
       "4  {'name': 'James Newton-King', 'email': '474ba6...     aspnet/AspNetCore   \n",
       "\n",
       "                                            language  \n",
       "0  [{'name': 'ASP', 'bytes': 109}, {'name': 'Batc...  \n",
       "1  [{'name': 'ASP', 'bytes': 109}, {'name': 'Batc...  \n",
       "2  [{'name': 'ASP', 'bytes': 109}, {'name': 'Batc...  \n",
       "3  [{'name': 'Batchfile', 'bytes': 34}, {'name': ...  \n",
       "4  [{'name': 'ASP', 'bytes': 109}, {'name': 'Batc...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "37581b51-7de6-42e5-b08b-b8c0196c61bd",
    "_uuid": "565d758bf0dfc13016a12bbff9c7118afc7a7a6a",
    "execution": {
     "iopub.execute_input": "2024-04-27T13:19:19.871509Z",
     "iopub.status.busy": "2024-04-27T13:19:19.871171Z",
     "iopub.status.idle": "2024-04-27T13:19:19.877843Z",
     "shell.execute_reply": "2024-04-27T13:19:19.876905Z",
     "shell.execute_reply.started": "2024-04-27T13:19:19.871450Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4186631, 4)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T13:29:58.355943Z",
     "iopub.status.busy": "2024-04-27T13:29:58.355549Z",
     "iopub.status.idle": "2024-04-27T13:32:51.560537Z",
     "shell.execute_reply": "2024-04-27T13:32:51.559611Z",
     "shell.execute_reply.started": "2024-04-27T13:29:58.355884Z"
    }
   },
   "outputs": [],
   "source": [
    "df.to_csv('Repo_Messages_Huge_Set.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6178,
     "sourceId": 337545,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 46,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
