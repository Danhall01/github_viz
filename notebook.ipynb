{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8677,"sourceType":"datasetVersion","datasetId":5836}],"dockerImageVersionId":8337,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This kernel will provide a brief overview of [the bq_helper package](https://github.com/SohierDane/BigQuery_Helper/blob/master/bq_helper.py), which simplifies common read-only tasks in BigQuery by dealing with object references and unpacking result objects into pandas dataframes.\n\nIt currently only works here on Kaggle as it does not have any handling for the BigQuery authorization functions that Kaggle handles behind the scenes.\n\nPlease note that the bq_helper API is **NOT** yet stable; it is a work in progress and I may introduce backwards incompatible changes.","metadata":{"_uuid":"e42f4e9f9d17a4cf289f29f904e93d4c52f59905","_cell_guid":"d2b72f9d-3d56-4f11-8cff-d4187de5ee58"}},{"cell_type":"code","source":"import pandas as pd\n# https://github.com/SohierDane/BigQuery_Helper\nfrom bq_helper import BigQueryHelper","metadata":{"_uuid":"b7de84dcf3864112fc6781f2bfab816b0de14488","_cell_guid":"e13c26e5-cd5f-4fad-bbbf-ed69334d7b2d","execution":{"iopub.status.busy":"2024-04-15T17:38:49.716750Z","iopub.execute_input":"2024-04-15T17:38:49.717256Z","iopub.status.idle":"2024-04-15T17:38:49.741311Z","shell.execute_reply.started":"2024-04-15T17:38:49.717198Z","shell.execute_reply":"2024-04-15T17:38:49.740569Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"bq_helper requires the creation of one BigQueryHelper object per dataset. Let's make one now. We'll need to pass it two arguments:\n- The name of the BigQuery project, which on Kaggle should always be `bigquery-public-data`\n- The name of the dataset, which can be found in the dataset description.","metadata":{"_uuid":"4a1523cfc09a2e97f6b32dbb5dd4509a45ef6241","_cell_guid":"11b91473-9bf0-464b-8d46-e8fae83f7323"}},{"cell_type":"code","source":"bq_assistant = BigQueryHelper(\"bigquery-public-data\", \"openaq\")","metadata":{"_uuid":"4e7f00cf78abc13cbefa9a172503a78eb10f70db","_cell_guid":"9f41459e-06a3-4a07-9126-8bcc98ee6ada","execution":{"iopub.status.busy":"2024-04-15T17:38:49.761133Z","iopub.execute_input":"2024-04-15T17:38:49.761618Z","iopub.status.idle":"2024-04-15T17:38:49.766247Z","shell.execute_reply.started":"2024-04-15T17:38:49.761571Z","shell.execute_reply":"2024-04-15T17:38:49.764835Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"The first thing I like to do with a dataset is to list all of the tables. This is a very small dataset, so there's just one.","metadata":{"_uuid":"b740f054bbfeed2eca36f54b8814eb24e2d2a514","_cell_guid":"5412b517-0bb7-4888-b8fe-d6abf53b0349"}},{"cell_type":"code","source":"bq_assistant.list_tables()","metadata":{"_uuid":"f02f02adca0137b16ef1d2e2bc75275a56b4807e","_cell_guid":"a5459a8d-8992-4414-b8f3-1dec0941de58","execution":{"iopub.status.busy":"2024-04-15T17:38:49.767780Z","iopub.execute_input":"2024-04-15T17:38:49.768371Z","iopub.status.idle":"2024-04-15T17:38:52.073814Z","shell.execute_reply.started":"2024-04-15T17:38:49.768068Z","shell.execute_reply":"2024-04-15T17:38:52.073053Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"['global_air_quality']"},"metadata":{}}]},{"cell_type":"markdown","source":"Basic EDA is the next obvious step. `head` is implemented using the efficient `list_rows` function, so it will never use much data. Essentially, BigQuery only ever scans the results you see, while comparable results with a `SELECT` query would need to scan the entire table. You can comfortably use `head` as many times as you want on tables of any size.","metadata":{"_uuid":"4ed754e546cee1aae2005f0e7a98e81a231349f5","_cell_guid":"7fa55f8d-86d4-4fe8-95ea-2b007564ecda"}},{"cell_type":"code","source":"bq_assistant.head(\"global_air_quality\", num_rows=3)","metadata":{"_uuid":"1c08260b07976ca1a45d75f1324254aea538806a","_cell_guid":"a0eed07d-c28b-4c5d-b022-46147346a8af","execution":{"iopub.status.busy":"2024-04-15T17:38:52.074858Z","iopub.execute_input":"2024-04-15T17:38:52.075240Z","iopub.status.idle":"2024-04-15T17:38:53.877480Z","shell.execute_reply.started":"2024-04-15T17:38:52.075197Z","shell.execute_reply":"2024-04-15T17:38:53.817757Z"},"trusted":true},"execution_count":4,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-5dcdedd48331>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbq_assistant\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"global_air_quality\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_rows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/src/bq-helper/bq_helper.py\u001b[0m in \u001b[0;36mhead\u001b[0;34m(self, table_name, num_rows, start_index, selected_columns)\u001b[0m\n\u001b[1;32m    168\u001b[0m         results = self.client.list_rows(active_table, selected_fields=schema_subset,\n\u001b[1;32m    169\u001b[0m             max_results=num_rows, start_index=start_index)\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         return pd.DataFrame(\n\u001b[1;32m    172\u001b[0m             data=[list(x.values()) for x in results], columns=list(results[0].keys()))\n","\u001b[0;32m/src/bq-helper/bq_helper.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    168\u001b[0m         results = self.client.list_rows(active_table, selected_fields=schema_subset,\n\u001b[1;32m    169\u001b[0m             max_results=num_rows, start_index=start_index)\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         return pd.DataFrame(\n\u001b[1;32m    172\u001b[0m             data=[list(x.values()) for x in results], columns=list(results[0].keys()))\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/google/api_core/page_iterator.py\u001b[0m in \u001b[0;36m_items_iter\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;34m\"\"\"Iterator for each item returned.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mpage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_page_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mincrement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpage\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_results\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/google/api_core/page_iterator.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;34m\"\"\"Get the next value in the page.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_item_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_item_to_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0;31m# Since we've successfully got the next value from the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;31m# iterator, we update the number of remaining.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/google/cloud/bigquery/table.py\u001b[0m in \u001b[0;36m_item_to_row\u001b[0;34m(iterator, resource)\u001b[0m\n\u001b[1;32m   1336\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mreturns\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mnext\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mpage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1337\u001b[0m     \"\"\"\n\u001b[0;32m-> 1338\u001b[0;31m     return Row(_helpers._row_tuple_from_json(resource, iterator.schema),\n\u001b[0m\u001b[1;32m   1339\u001b[0m                iterator._field_to_index)\n\u001b[1;32m   1340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/google/cloud/bigquery/_helpers.py\u001b[0m in \u001b[0;36m_row_tuple_from_json\u001b[0;34m(row, schema)\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0mrow_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfield\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'f'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m         \u001b[0mconverter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_CELLDATA_FROM_JSON\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfield\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfield_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfield\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'REPEATED'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m             row_data.append([converter(item['v'], field)\n","\u001b[0;31mKeyError\u001b[0m: 'GEOGRAPHY'"],"ename":"KeyError","evalue":"'GEOGRAPHY'","output_type":"error"}]},{"cell_type":"markdown","source":"It would be nice to get some more details about the columns, let's review the table schema.","metadata":{"_uuid":"33290cbece42d86c37c7df314cec290acc8aa7a1","_cell_guid":"7d44b7e1-037b-4206-9d88-ce527c0290db"}},{"cell_type":"code","source":"bq_assistant.table_schema(\"global_air_quality\")","metadata":{"_uuid":"50aba284bf31e3fd507bcec3b431cae492d0e680","_cell_guid":"75d4b5f0-be8e-41e1-a994-096cf130a55e","scrolled":true,"execution":{"iopub.status.busy":"2024-04-15T17:38:53.818892Z","iopub.status.idle":"2024-04-15T17:38:53.819597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we're ready to write a simple query. We should check how much memory it will scan. It won't matter in this case since the table is only 2 MB, but it's a good habit to get into for when you start working on larger datasets like [Github](https://www.kaggle.com/github/github-repos).","metadata":{"_uuid":"28650f926ac278ea564c56725f35a723047ddff3","_cell_guid":"c81619d0-e9f3-477a-bf33-4cb068bf23a6"}},{"cell_type":"code","source":"QUERY = \"SELECT location, timestamp, pollutant FROM `bigquery-public-data.openaq.global_air_quality`\"","metadata":{"_uuid":"7c605dc64497f5a9b4350a0fcb389ad53be21023","_cell_guid":"c0b7cf37-6c1c-4d80-af66-2651292b8780","execution":{"iopub.status.busy":"2024-04-15T17:38:53.820412Z","iopub.status.idle":"2024-04-15T17:38:53.820808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bq_assistant.estimate_query_size(QUERY)","metadata":{"_uuid":"4047740a362c748eae880f885eb31c33cb43e3ba","_cell_guid":"8126a9e4-6cb6-44ce-b257-ef053ec41c41","execution":{"iopub.status.busy":"2024-04-15T17:38:53.821293Z","iopub.status.idle":"2024-04-15T17:38:53.821894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"That's roughly 0.4 MB. Won't even make a dent in our resource allocation of 5 TB. \n\nNow we can move on to actually running the query. We'll get the results back as a pandas DataFrame.","metadata":{"_uuid":"2560a7ce0ffc89089a479a031bc58a6ab903d37c","_cell_guid":"27c7c438-a77c-4599-8a4b-5c877d1f1fae"}},{"cell_type":"code","source":"df = bq_assistant.query_to_pandas(QUERY)","metadata":{"_uuid":"767cbf8ce81cfc7920023095a6b67c78ed755303","_cell_guid":"48a6d390-2b2c-4b65-9382-e33dc4a28ebb","execution":{"iopub.status.busy":"2024-04-15T17:38:53.822469Z","iopub.status.idle":"2024-04-15T17:38:53.822928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head(3)","metadata":{"_uuid":"1aed7f686f8acac6546dc2d71617d9367b55d8ff","_cell_guid":"93e89fef-8000-487e-b285-7a174d28e57c","execution":{"iopub.status.busy":"2024-04-15T17:38:53.823578Z","iopub.status.idle":"2024-04-15T17:38:53.823939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If we were concerned about accidentally running large queries, there's a safe version of the query runner that will cancel large requests. In this first case, the query is well below the default scan limit of 1 GB and gets executed. ","metadata":{"_uuid":"62e1f82da17bc22485f0e782deb1c3312c1a323c","_cell_guid":"8490eb06-5a79-4165-87fd-dc3e299f8b1b"}},{"cell_type":"code","source":"df = bq_assistant.query_to_pandas_safe(QUERY)","metadata":{"_uuid":"99a1800feaa86a9f629447e46f619d78b82c7ca0","_cell_guid":"0a49d382-dcf5-4ec3-aee9-c7dc73e5d0a8","execution":{"iopub.status.busy":"2024-04-15T17:38:53.824532Z","iopub.status.idle":"2024-04-15T17:38:53.824979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If we reduce the allowed scan size to the (insanely low) level of 1 kilobyte, the query will never be run.","metadata":{"_uuid":"14a0698044edfbc9f54d54da1d917eceb169d597","_cell_guid":"dbf2cbfb-3938-4d63-b663-ab20a2fbb498"}},{"cell_type":"code","source":"df = bq_assistant.query_to_pandas_safe(QUERY, max_gb_scanned=1/10**6)","metadata":{"_uuid":"b56154ce29e09be1d548efa35cc1ddbe34fcdedf","_cell_guid":"4859cfe0-d752-4571-ad9c-1bc0633d8260","execution":{"iopub.status.busy":"2024-04-15T17:38:53.825528Z","iopub.status.idle":"2024-04-15T17:38:53.825873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Once you're ready to take full control of BigQuery and move beyond bq_helper, I'd recommend taking a look at both the [BigQuery API documentation](https://googlecloudplatform.github.io/google-cloud-python/latest/bigquery/reference.html) and the [bq_helper source code](https://github.com/SohierDane/BigQuery_Helper/blob/master/bq_helper.py). The [source code](https://github.com/SohierDane/BigQuery_Helper/blob/master/bq_helper.py) will help you understand what sections of the [BigQuery documentation](https://googlecloudplatform.github.io/google-cloud-python/latest/bigquery/reference.html) to read first.","metadata":{"_uuid":"f6d591e2b147d7db7a028fab398bdafe3611cdc2","_cell_guid":"2e30cf28-b7dc-4dc3-932e-205218447c80"}},{"cell_type":"code","source":"","metadata":{"_uuid":"092d9f198821e7065e42ddc9eb17a9bd9992981b","_cell_guid":"2530ae1b-559a-4f7f-9ed4-41d7275eb8b3","trusted":true},"execution_count":null,"outputs":[]}]}